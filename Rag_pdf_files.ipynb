{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vf7KMm58PIpt"
      ],
      "authorship_tag": "ABX9TyNPATlB5hcJm/OCeSCbJIVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamBelg/RAG/blob/main/Rag_pdf_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "vf7KMm58PIpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UxmeHNKOrg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de2a3e3-d2cb-414e-9bce-cb2de53e2ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.14-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m970.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_core\n",
            "  Downloading langchain_core-0.2.11-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.32.0 (from langchain_openai)\n",
            "  Downloading openai-1.35.9-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain_core)\n",
            "  Downloading langsmith-0.1.83-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.4.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Collecting pinecone-client<4.0.0,>=3.2.2 (from langchain_pinecone)\n",
            "  Downloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.32.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2024.6.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.2.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pypdf2, pypdf, pinecone-client, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain_core, langchain-text-splitters, langchain_pinecone, langchain_openai, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-text-splitters-0.2.2 langchain_community-0.2.6 langchain_core-0.2.11 langchain_openai-0.1.14 langchain_pinecone-0.1.1 langsmith-0.1.83 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.35.9 orjson-3.10.6 pinecone-client-3.2.2 pypdf-4.2.0 pypdf2-3.0.1 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai langchain_core langchain_community langchain_pinecone langchain pypdf2 pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read PDF"
      ],
      "metadata": {
        "id": "2JAiG55n87YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_link = \"https://arxiv.org/pdf/2402.16893\"\n",
        "loader = PyPDFLoader(file_path=pdf_link,\n",
        "                     extract_images=False)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 4000, chunk_overlap= 50)\n",
        "pages = loader.load_and_split(text_splitter)"
      ],
      "metadata": {
        "id": "FRGdZ-NXO8Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcaSWyS-5CFh",
        "outputId": "328e48b8-3769-4b19-bbca-268b32389fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://arxiv.org/pdf/2402.16893', 'page': 0}, page_content='The Good and The Bad: Exploring Privacy Issues\\nin Retrieval-Augmented Generation (RAG)\\nShenglai Zeng1*†, Jiankun Zhang∗3,4,5, Pengfei He1, Yue Xing1, Yiding Liu2, Han Xu1\\nJie Ren1, Shuaiqiang Wang2, Dawei Yin2, Yi Chang3,4,5, Jiliang Tang1\\n1Michigan State University2Baidu, Inc.\\n3School of Artificial Intelligence, Jilin University\\n4International Center of Future Science, Jilin University\\n5Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China\\nAbstract\\nRetrieval-augmented generation (RAG) is a\\npowerful technique to facilitate language model\\nwith proprietary and private data, where data\\nprivacy is a pivotal concern. Whereas extensive\\nresearch has demonstrated the privacy risks of\\nlarge language models (LLMs), the RAG tech-\\nnique could potentially reshape the inherent\\nbehaviors of LLM generation, posing new pri-\\nvacy issues that are currently under-explored.\\nIn this work, we conduct extensive empiri-\\ncal studies with novel attack methods, which\\ndemonstrate the vulnerability of RAG systems\\non leaking the private retrieval database. De-\\nspite the new risk brought by RAG on the re-\\ntrieval data, we further reveal that RAG can\\nmitigate the leakage of the LLMs’ training\\ndata. Overall, we provide new insights in\\nthis paper for privacy protection of retrieval-\\naugmented LLMs, which benefit both LLMs\\nand RAG systems builders. Our code is avail-\\nable at https://github.com/phycholosogy/RAG-\\nprivacy.\\n1 Introduction\\nRetrieval-augmented generation (RAG) (Liu, 2022;\\nChase, 2022; Van Veen et al., 2023; Ram et al.,\\n2023; Shi et al., 2023) is an advanced natural lan-\\nguage processing technique that enhances text gen-\\neration by integrating information retrieved from\\na large corpus of documents. These techniques\\nenable RAG to produce accurate and contextually\\nrelevant outputs with augmented external knowl-\\nedge and have been widely used in various scenar-\\nios such as domain-specific chatbots (Siriwardhana\\net al., 2023) and email/code completion (Parvez\\net al., 2021). RAG systems typically work in two\\nphases, as shown in Fig 1 - retrieval and generation.\\nWhen a user query is entered, relevant knowledge\\nis first retrieved from an external database. The\\nretrieved data is then combined with the original\\n*Equal contribution.\\n†Corresponding to zengshe1@msu.edu\\nQuery\\nRetrievalDBRelevantDocs\\nResponse\\nTrainingDataAttackerEmbeddingModelE\\nLLMs\\nLeakageQQueryRetrievalAugmentedGenerationFigure 1: The RAG system and potential risks.\\nquery to form the input to a large language model\\n(LLM). The LLM then uses its pre-trained knowl-\\nedge and the retrieved data to generate a response.\\nIn this paper, we focus on studying the risk of\\nprivacy leakage in the RAG system, and we argue\\nthat the information from both retrieval dataset and\\nthe pre-training/fine-tuning dataset (of the LLM)\\nare potential to be released by RAG usage. On\\none hand , the retrieval dataset can contain sensi-\\ntive, valuable domain-specific information (Parvez\\net al., 2021; Kulkarni et al., 2024), such as patients\\nprescriptions can be used for RAG-based medical\\nchatbots (Yunxiang et al., 2023). On the other\\nhand , the retrieval process in RAG could also influ-\\nence the behavior of the LLMs for text-generation,\\nand this could possibly cause the LLMs to output\\nprivate information from its training/fine-tuning\\ndataset. Notably, there are existing works (Car-\\nlini et al., 2021; Kandpal et al., 2022; Lee et al.,\\n2021; Carlini et al., 2022; Zeng et al., 2023) ob-\\nserving that LLMs can remember and leak private\\ninformation from their pre-training and fine-tuning\\ndata. However, how the integration of external re-\\ntrieval data can affect the memorization behavior\\nof LLMs in RAG is still unclear and worth further\\nexploration. Therefore, these concerns motivate us\\nto answer the research questions:\\n•(RQ1) Can we extract private data from the\\nexternal retrieval database in RAG?arXiv:2402.16893v1  [cs.CR]  23 Feb 2024')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "KY07WYkR8zAE",
        "outputId": "2b78281b-268d-4ef1-c66c-a1100c52a62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Good and The Bad: Exploring Privacy Issues\\nin Retrieval-Augmented Generation (RAG)\\nShenglai Zeng1*†, Jiankun Zhang∗3,4,5, Pengfei He1, Yue Xing1, Yiding Liu2, Han Xu1\\nJie Ren1, Shuaiqiang Wang2, Dawei Yin2, Yi Chang3,4,5, Jiliang Tang1\\n1Michigan State University2Baidu, Inc.\\n3School of Artificial Intelligence, Jilin University\\n4International Center of Future Science, Jilin University\\n5Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China\\nAbstract\\nRetrieval-augmented generation (RAG) is a\\npowerful technique to facilitate language model\\nwith proprietary and private data, where data\\nprivacy is a pivotal concern. Whereas extensive\\nresearch has demonstrated the privacy risks of\\nlarge language models (LLMs), the RAG tech-\\nnique could potentially reshape the inherent\\nbehaviors of LLM generation, posing new pri-\\nvacy issues that are currently under-explored.\\nIn this work, we conduct extensive empiri-\\ncal studies with novel attack methods, which\\ndemonstrate the vulnerability of RAG systems\\non leaking the private retrieval database. De-\\nspite the new risk brought by RAG on the re-\\ntrieval data, we further reveal that RAG can\\nmitigate the leakage of the LLMs’ training\\ndata. Overall, we provide new insights in\\nthis paper for privacy protection of retrieval-\\naugmented LLMs, which benefit both LLMs\\nand RAG systems builders. Our code is avail-\\nable at https://github.com/phycholosogy/RAG-\\nprivacy.\\n1 Introduction\\nRetrieval-augmented generation (RAG) (Liu, 2022;\\nChase, 2022; Van Veen et al., 2023; Ram et al.,\\n2023; Shi et al., 2023) is an advanced natural lan-\\nguage processing technique that enhances text gen-\\neration by integrating information retrieved from\\na large corpus of documents. These techniques\\nenable RAG to produce accurate and contextually\\nrelevant outputs with augmented external knowl-\\nedge and have been widely used in various scenar-\\nios such as domain-specific chatbots (Siriwardhana\\net al., 2023) and email/code completion (Parvez\\net al., 2021). RAG systems typically work in two\\nphases, as shown in Fig 1 - retrieval and generation.\\nWhen a user query is entered, relevant knowledge\\nis first retrieved from an external database. The\\nretrieved data is then combined with the original\\n*Equal contribution.\\n†Corresponding to zengshe1@msu.edu\\nQuery\\nRetrievalDBRelevantDocs\\nResponse\\nTrainingDataAttackerEmbeddingModelE\\nLLMs\\nLeakageQQueryRetrievalAugmentedGenerationFigure 1: The RAG system and potential risks.\\nquery to form the input to a large language model\\n(LLM). The LLM then uses its pre-trained knowl-\\nedge and the retrieved data to generate a response.\\nIn this paper, we focus on studying the risk of\\nprivacy leakage in the RAG system, and we argue\\nthat the information from both retrieval dataset and\\nthe pre-training/fine-tuning dataset (of the LLM)\\nare potential to be released by RAG usage. On\\none hand , the retrieval dataset can contain sensi-\\ntive, valuable domain-specific information (Parvez\\net al., 2021; Kulkarni et al., 2024), such as patients\\nprescriptions can be used for RAG-based medical\\nchatbots (Yunxiang et al., 2023). On the other\\nhand , the retrieval process in RAG could also influ-\\nence the behavior of the LLMs for text-generation,\\nand this could possibly cause the LLMs to output\\nprivate information from its training/fine-tuning\\ndataset. Notably, there are existing works (Car-\\nlini et al., 2021; Kandpal et al., 2022; Lee et al.,\\n2021; Carlini et al., 2022; Zeng et al., 2023) ob-\\nserving that LLMs can remember and leak private\\ninformation from their pre-training and fine-tuning\\ndata. However, how the integration of external re-\\ntrieval data can affect the memorization behavior\\nof LLMs in RAG is still unclear and worth further\\nexploration. Therefore, these concerns motivate us\\nto answer the research questions:\\n•(RQ1) Can we extract private data from the\\nexternal retrieval database in RAG?arXiv:2402.16893v1  [cs.CR]  23 Feb 2024'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u8fI8ku5Pmk",
        "outputId": "7eb65457-df09-45be-f7d8-8c1bcd9493bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG model (gpt 3.5)"
      ],
      "metadata": {
        "id": "0nGf_pAb_W1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Opem ai model (gpt 3.5)\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get(\"open_ai\")\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
        "                   model=\"gpt-3.5-turbo\",\n",
        "                   temperature=0.2)"
      ],
      "metadata": {
        "id": "Cluioeew_XVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open ai embeddings\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "embeddings =OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "yuaxdSgJ_s50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template (context + question)\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "template = \"\"\"\n",
        "Answer the question based on the content below. If you can't answer the question, reply 'I don't know'.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "uMSfo3_i_hyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "OYVEcitM_oLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pinecone for cosine distance between question and different documents\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "import os\n",
        "index_name = \"pdf-files\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"pinecone_api_key\")\n",
        "\n",
        "pinecone = PineconeVectorStore.from_documents(\n",
        "    pages, embedding=embeddings, index_name=index_name\n",
        ")"
      ],
      "metadata": {
        "id": "mtkiR8ij_8sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\":pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is the definition of RAG?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q92JDoOvABpO",
        "outputId": "934d53f6-d9f0-4a2a-a2d6-6bb6ed41deec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval-augmented generation (RAG) is an advanced natural language processing technique that enhances text generation by integrating information retrieved from a large corpus of documents.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\":pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What are the different risks of using RAG ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k3T4ingQUXdk",
        "outputId": "acfd9c73-1cc2-4680-8a4f-f36c6a78f46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The different risks of using RAG include the potential extraction of private data from the external retrieval database and the possibility of the LLMs outputting private information from their training/fine-tuning dataset.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\":pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is the best strategy to deal with private data?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1OUyQaU9ENw2",
        "outputId": "6403b189-fd20-4f42-83d8-bc8dba868569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the content provided, the best strategy to deal with private data involves conducting targeted attacks to extract specific private information while also evaluating the memorization effects through prefix attacks. Additionally, exploring various defense techniques to mitigate privacy risks associated with retrieval-augmented generation (RAG) techniques is crucial.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\":pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"Who are the authors of this paper ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YeuFKDS0FCOI",
        "outputId": "2876bf97-5617-4e3b-c6f6-559bc2d8ce2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\":pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What are the refrences of this paper\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "yzwlw8BNFSHo",
        "outputId": "7d3db2de-0d4f-4ab4-96ce-e67164c02d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The references of this paper include:\\n1. Jooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee. 2023. Do language models plagiarize? In Proceedings of the ACM Web Conference 2023, pages 3637–3647.\\n2. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499.\\n3. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474.\\n4. Liu. 2023. Twitter post. https://twitter.com/kliu128/status/1623472922374574080.\\n5. Jerry Liu. 2022. Llamaindex. 11 2022. https://github.com/jerryjliu/llama_index.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize"
      ],
      "metadata": {
        "id": "FMNeznzKJjWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "chain = load_summarize_chain(model, chain_type=\"stuff\")\n",
        "result = chain.invoke(pages)\n",
        "\n",
        "print(result[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzLiWyuQJRaG",
        "outputId": "1a4078ae-d94a-496b-cebb-73d85d2c840c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The study explores privacy issues in Retrieval-Augmented Generation (RAG) systems, focusing on the vulnerability of leaking private retrieval database information and the impact of retrieval data on the memorization behavior of large language models (LLMs). The research reveals that RAG systems are susceptible to privacy risks but can also mitigate the leakage of training data from LLMs. Various attack methods and defense strategies are examined, showing that integrating retrieval data can reduce the risk of leaking training data. The study provides insights for privacy protection in RAG systems.\n"
          ]
        }
      ]
    }
  ]
}